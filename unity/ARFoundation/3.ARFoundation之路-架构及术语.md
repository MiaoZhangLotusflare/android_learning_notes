# 3.ARFoundation之路-架构及术语

学习文章地址：[ARFoundation之路－架构及术语_DavidWang-CSDN博客](https://davidwang.blog.csdn.net/article/details/91866650)

## ARFoundation 功能概述

ARFoundation 能实现的功能也与底层 SDK 所处的平台相关，如 ARKit 有 worldmap 功能，而 ARCore 没有，这时，即使 ARFoundation 支持 worldmap 功能，其也只能在 ARKit 运行的 iOS 平台这个功能才有效，编译到 Android 平台就会出错。这即是说 ARFoundation 支持的功能与底层 SDK 是密切相关的，脱离底层 SDK 谈 ARFoundation 功能是没有意义的。当然，如果 ARKit 和 ARCore 都支持的功能，ARFoundation 做的工作是在编译时根据平台选择无缝切换所用底层 SDK，达到一次开发，跨平台部署的目的。

当前，ARFoundation 主要支持以下功能：

| 功能                     | 描述                                                                          |
| ---------------------- | --------------------------------------------------------------------------- |
| 世界跟踪（World tracking）   | 在物理空间中跟踪用户设备的位置和方向（姿态）。                                                     |
| 平面检测（Plane detection）  | 对水平与垂直平面进行检测。                                                               |
| 参考点（Reference points）  | 对特定点的姿态跟踪。ARCore 中称为 Anchor。                                                |
| 光照估计（Light estimation） | 对物理环境中的光照强弱及方向进行估计。                                                         |
| 人脸跟踪（Face tracking）    | 检测并跟踪人脸。                                                                    |
| 图像跟踪（Image tracking）   | 跟踪物理空间中的 2D 图像。                                                             |
| 物体跟踪（Object tracking）  | 跟踪物理空间中的物体对象，目前只支持 ARKit。                                                   |
| Seesion 分享             | 支持多人共享场景，这在 ARKit 中称为多人协作（Collaborative session），在 ARCore 中称为 Cloud Anchor。 |
| 人体动作捕捉（Motion capture） | 简称动捕，检测物理空间中的人体及动作，用一个由 17 根骨骼组成的层次关节来表达人体动作。                               |
| 人形遮挡（People occlusion） | 利用计算机视觉判断人体在场景中的位置，获取人体形状及在场景中的位置实现虚拟物体遮挡。                                  |
| 摄像机图像 API              | 提供摄像机图像底层支持，方便开发人员开发计算机视觉应用。                                                |

## ARFoundation 架构体系

虽然 ARFoundtion 是在底层 SDK API 之上的再次封装，但 Unity 为了实现 AR 跨平台应用（平台无关性）做了大量工作，搭建了一个开放性的架构体系，使这个架构能够容纳各类底层 SDK，能支持当前及以后其他底层 AR SDK 的加入，宏观上看，ARFoundation 希望构建一个开发各类 AR 应用的统一平台。

为实现这个开放的架构，ARFoundation 建立在一系列的子系统（subsystem）之上。subsystem 隶属于 UnityEngine.XR.ARSubsystems 命名空间，负责实现特定的功能模块，而且这个实现与平台无关，即 subsystem 处理与平台相关的特定模块的实现。如 XRPlaneSubsystem 负责实现平面检测、显示功能，不仅如此，其还要根据运行平台的不同自动的调用不同底层的 SDK，从调用者的角度看，他只调用了 XRPlaneSubsystem 的功能，而不用去管最终这个实现是基于 iOS 还是 Android，即对平台透明。

这种架构对上提供了与平台无关的功能，对下可以在以后的发展中纳入不同的底层 SDK，从而实现最终的一次开发，跨平台部署的目标。其架构图如下所示：

![](img/架构.jpg)

## 基本术语

### 世界跟踪（Tracking）

指 AR 设备确定其在物理世界中的相对位置和方向的能力。在 2D 和 3D 空间中跟踪用户的运动并最终定位它们的位置是任何 AR 应用程序的基础，当我们的移动设备在现实世界中移动时，ARFoundation 会通过一个名为并行测距与映射（Concurrent Odometry and Mapping ，COM）的过程来理解移动设备相对于周围世界的位置。 ARFoundation 会检测捕获的摄像头图像中的视觉差异特征（称为特征点），并使用这些点来计算其位置变化。 这些视觉信息将与设备 IMU 的惯性测量结果结合，一起用于估测摄像头随着时间推移而相对于周围世界的姿态（位置和方向）。

在开发中，通过将渲染 3D 内容的虚拟摄像头的姿态与 ARFoundation 提供的设备摄像头的姿态对齐，开发者能够从正确的透视角度渲染虚拟内容，渲染的虚拟图像可以叠加到从设备摄像头获取的图像上，让虚拟内容看起来就像现实世界的一部分一样。

### 可跟踪（Trackable）

可以被 AR 设备检测或跟踪的真实特征，例如特征点，平面，人脸，人形等等。

### 特征点（Feature Point）

AR 设备使用摄像机和图像分析来跟踪世界上用于构建环境地图的特定点，例如木纹表面的结。特征点云包含了被观察到的 3D 点和视觉特异点的集合，通常还附有检测时的时间戳。

### 会话（Session）

  Session 的功能是管理 AR 系统的状态，处理 Session 生命周期，是 AR API 的主要入口。在开始使用 AR API 的时候，通过设置的 ARSessionState 来检查当前设备是否支持 AR。Session 负责处理整个 AR 应用的生命周期，这样 AR 系统会根据需要开始和暂停相机帧的采集，初始化和释放相关的资源。
  

### Session 空间（Session space）

相对于 AR session 初始化时的坐标系，例如，例如，会话空间（0，0，0）是指创建 AR 会话的位置。AR 设备跟踪的坐标信息都是处在 Session 空间中，因此，在使用时，需要将其从 Session 空间转换到其他空间。这类似于模型空间和世界空间的转换。

### 用户交互（User interaction）

ARFoundation 利用命中测试来获取对应于手机屏幕的 (x,y) 坐标（通过点按或应用支持的任何其他交互提供），并将一条射线投影到摄像头的视野中，返回这条射线贯穿的任何平面或特征点以及交叉位置在现实世界空间中的姿态。 这让用户可以选择环境中的物体或者与它们互动。

### 增强图像（Augumented Image）

使用增强图像（图像检测）可以构建能够响应特定 2D 图像（如产品包装或电影海报）的 AR 应用， 用户可以将手机的摄像头对准特定图像时触发 AR 体验，例如，他们可以将手机的摄像头对准电影海报，使人物弹出，然后引发一个场景。可离线编译图像以创建图像数据库，也可以从设备实时添加单独的图像。 注册后，ARFoundation 将检测这些图像、图像边界，然后返回相应的姿态。

### 共享 （Sharing）

借助于 ARKit 中的多人协作（Collaborative session）或者 ARCore 中的 Cloud Anchor，可以创建适用于 Android 和 iOS 设备的协作性或多人游戏应用。使用云锚点，一台设备可以将锚点和附近的特征点发送到云端进行托管。 可以将这些锚点与同一环境中 Android 或 iOS 设备上的其他用户共享。 这使应用可以渲染连接到这些锚点的相同 3D 对象，从而让用户能够同步拥有相同的 AR 体验。共享需要服务器的支持。

### 平面（Plane）

AR 中所有的内容，都要依托于平面类进行渲染。如虚拟机器人，只有在检测到平面网格的地方才能放置。平面可分为水平、垂直两种，Plane 描述了对一个真实世界二维平面的认知，如平面的中心点、平面的 x 和 z 轴方向长度，组成平面多边形的顶点。检测到的平面还分为三种状态，分别是正在跟踪、可恢复跟踪和永不恢复跟踪。如果是没有正在跟踪的平面，包含的平面信息可能不准确。两个或者多个平面还会被被自动合并成一个父平面。如果这种情况发生，可以通过子平面找到它的父平面。

### 姿态（Pose）

Pose 表示从一个坐标系到另一个坐标系的转换。在所有的 ARFoundation APIs 中，Pose 总是描述从物体的局部坐标系到世界坐标系的变换，也就是说，来自 ARFoundation API 的 Pose 可以被认为等同于 OpenGL 的模型矩阵或 DirectX 的世界矩阵。随着 ARFoundation 对环境的了解不断变化，它将调整坐标系模式以便与真实世界保持一致。 这时，Camera 的位置（坐标）可能会发生明显的变化，以便它们所代表的物体处理恰当的位置。因此，每一帧图像都应被认为是在一个完全独立的世界坐标空间中。

### 光照估计（LightEstimate）

LightEstimate 给我们提供了一个接口来查询当前帧的光照环境。我们可以获取当前相机视图的光照强度，一个范围在（0.0，1.0）的值，0 代表黑色，1 代表白色，使用该光照信息绘制内容，可以使虚拟物体更真实。还可以获取到光照方向，以便调整 AR 中虚拟物体的阴影方向，增加虚拟物体的真实感。
